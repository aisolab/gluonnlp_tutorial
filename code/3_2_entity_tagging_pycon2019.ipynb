{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "entity_tagging.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xex7CIsaIZms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a2728546-990f-407b-8b17-8956e47a0335"
      },
      "source": [
        "!pip install mxnet-cu100\n",
        "!pip install gluonnlp pandas tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.16.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (1.24.3)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.16.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZPM9avIXjH",
        "colab_type": "text"
      },
      "source": [
        "## Entity Taggging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS9E3KCwIXjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mxnet.gluon import nn, rnn\n",
        "from mxnet import gluon, autograd\n",
        "import gluonnlp as nlp\n",
        "from mxnet import nd \n",
        "import mxnet as mx\n",
        "import time\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import multiprocessing as mp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rei93JgIXja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_raw = pd.read_csv(\"https://www.dropbox.com/s/83n0uoy20rd2vq4/trainset.txt?dl=1\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
        "#validation_raw = pd.read_csv(\"https://www.dropbox.com/s/kbl7kw54jdo2550/test_hidden.txt?dl=1\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
        "validation_raw = pd.read_csv(\"https://www.dropbox.com/s/enxp9yt9cstcal2/validation.txt?dl=1\",names=['intent', 'entity', 'sentence'], sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up8TL-t9IXjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "0c6416b8-25ec-4652-f4e3-7a288567609d"
      },
      "source": [
        "train_raw.head(30)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intent</th>\n",
              "      <th>entity</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>area</td>\n",
              "      <td>EECCCCCCCCCCCCCCCCCCC</td>\n",
              "      <td>자강의 면적은 얼마 정도되는지 알려줄래</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>birth_date</td>\n",
              "      <td>CCCCCCCCCCCCEEECCCCCCCCCCCC</td>\n",
              "      <td>WIKI PEDIA로 변재일 생년월일을 알고 싶어</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>age</td>\n",
              "      <td>EEEEEEEEEEECCCCCCCCCCCCCCCCC</td>\n",
              "      <td>남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>length</td>\n",
              "      <td>EEEECCCCCCCCCCCCCCCCCC</td>\n",
              "      <td>삼양터널의 총 길이 위키백과사전에서 뭐야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>birth_place</td>\n",
              "      <td>EEEEEECCCCCCCCCCC</td>\n",
              "      <td>코니 윌리스의 태어난 곳은 뭐지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>weight</td>\n",
              "      <td>CCCCCCCCCCCCEEEECCCCCCCCCCCCC</td>\n",
              "      <td>WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>definition</td>\n",
              "      <td>CCCCCCCCCCCCCEEECCCCCCCC</td>\n",
              "      <td>WIKIPEDIA백과로 라이프 찾아서 말해줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>height</td>\n",
              "      <td>EEEEEEEECCCCCCCCCCCCCCCCCCC</td>\n",
              "      <td>송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>birth_date</td>\n",
              "      <td>CCCEEEEEECCCCCCCCCCCCCCC</td>\n",
              "      <td>검색 HLKVAM 언제 출생했는지를 검색해라</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>height</td>\n",
              "      <td>CCCCCCCCEEEEEECCCCCCCC</td>\n",
              "      <td>위키 피디아에 푸조 508 전고가 몇이야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>length</td>\n",
              "      <td>CCCEEEEECCCCCCC</td>\n",
              "      <td>검색 호몬혼 섬 길이를 찾아</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>definition</td>\n",
              "      <td>EEEEECCCCCCCCCCCCC</td>\n",
              "      <td>영산중학교 좀 위키피디아사전 검색</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>age</td>\n",
              "      <td>CCCCCCEEEEEECCCCCCC</td>\n",
              "      <td>위키백과로 침보라조 산 나이 어떤지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>birth_date</td>\n",
              "      <td>EEEEEEECCCCCCCC</td>\n",
              "      <td>마무드 아스라의 출생 찾아줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>birth_place</td>\n",
              "      <td>CCCCCCEEEEEEECCCCCCCCC</td>\n",
              "      <td>위키 백과 조제 카리오카의 출생지를 찾아</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>birth_date</td>\n",
              "      <td>CCCEEEEEECCCCCCCCC</td>\n",
              "      <td>검색 제이 개츠비 생년월일은 뭐지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>length</td>\n",
              "      <td>EEEECCCCCCCCCCCCCCCCC</td>\n",
              "      <td>증약터널의 길이가 얼마쯤인지 혹시 알아</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>belong_to</td>\n",
              "      <td>EEEEEEEEEEEEEEEECCCCCCCCCCCCCC</td>\n",
              "      <td>리히텐슈타인의 한스 아담 2세 소속사는 어딘지 검색해봐</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>height</td>\n",
              "      <td>CCCCCCCCCCCCEEEEECCCCCCCCC</td>\n",
              "      <td>WIKI사전백과 검색 벨록스여우의 높이는 얼만지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>age</td>\n",
              "      <td>EEEEEECCCCCCCCC</td>\n",
              "      <td>파블롭스키구의 나이를 찾아줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>width</td>\n",
              "      <td>EEEEEEECCCCCCCCCCCCCC</td>\n",
              "      <td>사카피솔라 섬의 너비는 WIKI에서 뭐</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>birth_place</td>\n",
              "      <td>EECCCCCCCCCCCCCCCCC</td>\n",
              "      <td>나미는 태어난 곳이 WIKI로 뭔지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>weight</td>\n",
              "      <td>CCCCCEEEEECCCCCCC</td>\n",
              "      <td>위키에서 피니스테르의 무게 찾기</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>birth_place</td>\n",
              "      <td>CCCEEEEEEECCCCCCCCCCC</td>\n",
              "      <td>검색 카를 야스퍼스 출신지역이 어디라고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>width</td>\n",
              "      <td>EEEEEEEEEEECCCCCCC</td>\n",
              "      <td>63식 병력수송장갑차의 폭 얼만지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>birth_place</td>\n",
              "      <td>CCCCCEEECCCCCCCCCCCC</td>\n",
              "      <td>검색으로 강마에가 출생 장소를 찾아줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>birth_date</td>\n",
              "      <td>EEEEEECCCCCCCCCCCCCC</td>\n",
              "      <td>쿠죠 히카리의 언제 출생했는지 탐색해</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>length</td>\n",
              "      <td>EEECCCCCCCCCCC</td>\n",
              "      <td>사하라의 길이가 얼마쯤이지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>area</td>\n",
              "      <td>EEEECCCCCCCCC</td>\n",
              "      <td>송대산성의 면적은 얼만지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>area</td>\n",
              "      <td>CCCCCCCCCCEEEEEECCCCCCC</td>\n",
              "      <td>WIKI 피디아에 신자경선생묘의 넓이 뭔지</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         intent                          entity                        sentence\n",
              "0          area           EECCCCCCCCCCCCCCCCCCC           자강의 면적은 얼마 정도되는지 알려줄래\n",
              "1    birth_date     CCCCCCCCCCCCEEECCCCCCCCCCCC     WIKI PEDIA로 변재일 생년월일을 알고 싶어\n",
              "2           age    EEEEEEEEEEECCCCCCCCCCCCCCCCC    남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야\n",
              "3        length          EEEECCCCCCCCCCCCCCCCCC          삼양터널의 총 길이 위키백과사전에서 뭐야\n",
              "4   birth_place               EEEEEECCCCCCCCCCC               코니 윌리스의 태어난 곳은 뭐지\n",
              "5        weight   CCCCCCCCCCCCEEEECCCCCCCCCCCCC   WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐\n",
              "6    definition        CCCCCCCCCCCCCEEECCCCCCCC        WIKIPEDIA백과로 라이프 찾아서 말해줘\n",
              "7        height     EEEEEEEECCCCCCCCCCCCCCCCCCC     송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야\n",
              "8    birth_date        CCCEEEEEECCCCCCCCCCCCCCC        검색 HLKVAM 언제 출생했는지를 검색해라\n",
              "9        height          CCCCCCCCEEEEEECCCCCCCC          위키 피디아에 푸조 508 전고가 몇이야\n",
              "10       length                 CCCEEEEECCCCCCC                 검색 호몬혼 섬 길이를 찾아\n",
              "11   definition              EEEEECCCCCCCCCCCCC              영산중학교 좀 위키피디아사전 검색\n",
              "12          age             CCCCCCEEEEEECCCCCCC             위키백과로 침보라조 산 나이 어떤지\n",
              "13   birth_date                 EEEEEEECCCCCCCC                 마무드 아스라의 출생 찾아줘\n",
              "14  birth_place          CCCCCCEEEEEEECCCCCCCCC          위키 백과 조제 카리오카의 출생지를 찾아\n",
              "15   birth_date              CCCEEEEEECCCCCCCCC              검색 제이 개츠비 생년월일은 뭐지\n",
              "16       length           EEEECCCCCCCCCCCCCCCCC           증약터널의 길이가 얼마쯤인지 혹시 알아\n",
              "17    belong_to  EEEEEEEEEEEEEEEECCCCCCCCCCCCCC  리히텐슈타인의 한스 아담 2세 소속사는 어딘지 검색해봐\n",
              "18       height      CCCCCCCCCCCCEEEEECCCCCCCCC      WIKI사전백과 검색 벨록스여우의 높이는 얼만지\n",
              "19          age                 EEEEEECCCCCCCCC                 파블롭스키구의 나이를 찾아줘\n",
              "20        width           EEEEEEECCCCCCCCCCCCCC           사카피솔라 섬의 너비는 WIKI에서 뭐\n",
              "21  birth_place             EECCCCCCCCCCCCCCCCC             나미는 태어난 곳이 WIKI로 뭔지\n",
              "22       weight               CCCCCEEEEECCCCCCC               위키에서 피니스테르의 무게 찾기\n",
              "23  birth_place           CCCEEEEEEECCCCCCCCCCC           검색 카를 야스퍼스 출신지역이 어디라고\n",
              "24        width              EEEEEEEEEEECCCCCCC              63식 병력수송장갑차의 폭 얼만지\n",
              "25  birth_place            CCCCCEEECCCCCCCCCCCC            검색으로 강마에가 출생 장소를 찾아줘\n",
              "26   birth_date            EEEEEECCCCCCCCCCCCCC            쿠죠 히카리의 언제 출생했는지 탐색해\n",
              "27       length                  EEECCCCCCCCCCC                  사하라의 길이가 얼마쯤이지\n",
              "28         area                   EEEECCCCCCCCC                   송대산성의 면적은 얼만지\n",
              "29         area         CCCCCCCCCCEEEEEECCCCCCC         WIKI 피디아에 신자경선생묘의 넓이 뭔지"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRSAAujAIXj0",
        "colab_type": "text"
      },
      "source": [
        "#### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLJgprxPIXj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = [(l, d) for d,l in zip(train_raw['entity'], train_raw['sentence'])]\n",
        "valid_dataset = [(l, d) for d,l in zip(validation_raw['entity'], validation_raw['sentence'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOByF7V-IXj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 32\n",
        "\n",
        "length_clip = nlp.data.PadSequence(seq_len, pad_val=\"<pad>\")\n",
        "\n",
        "def preprocess(data):\n",
        "    sent, entity = data\n",
        "    char_sent = list(str(sent))\n",
        "    char_entity = list(str(entity))\n",
        "    return(length_clip(char_sent), len(sent),length_clip(char_entity))\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    start = time.time()\n",
        "    with mp.Pool() as pool:\n",
        "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
        "    end = time.time()\n",
        "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
        "          .format(end - start, len(dataset)))\n",
        "    return dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XZMULFiIXkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "355ba816-6bfc-4c70-8503-08fab70f2f66"
      },
      "source": [
        "train_preprocessed  = preprocess_dataset(train_dataset)\n",
        "valid_preprocessed  = preprocess_dataset(valid_dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done! Tokenizing Time=0.34s, #Sentences=9000\n",
            "Done! Tokenizing Time=0.13s, #Sentences=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y5xFJOKIXkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter_sent   = nlp.data.count_tokens(itertools.chain.from_iterable([c for c, _, _ in train_preprocessed]))\n",
        "counter_entity = nlp.data.count_tokens(itertools.chain.from_iterable([c for _,_, c in train_preprocessed]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_54U3R_IXka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_sent = nlp.Vocab(counter_sent, bos_token=None, eos_token=None, min_freq=15)\n",
        "vocab_entity = nlp.Vocab(counter_entity, bos_token=None, eos_token=None, unknown_token=None ,min_freq=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ8pwPwXIXkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a122c09c-9300-4bee-fb76-ab7292141336"
      },
      "source": [
        "vocab_sent.idx_to_token[:10], vocab_entity.idx_to_token[:10], "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<unk>', '<pad>', ' ', 'I', '이', '색', '검', '의', '지', '아'],\n",
              " ['<pad>', 'C', 'E'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhkhqOGNIXkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_preprocessed_encoded  = [(vocab_sent[sent], length ,vocab_entity[entity])  for sent, length ,entity in train_preprocessed ]\n",
        "valid  = [(vocab_sent[sent], length ,vocab_entity[entity])  for sent, length ,entity in valid_preprocessed ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me3AVOOeIXk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = nlp.data.train_valid_split(train_preprocessed_encoded, valid_ratio=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhp6acqIXlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbatch = 30\n",
        "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Stack(),\n",
        "                                      nlp.data.batchify.Stack('float32'),\n",
        "                                      nlp.data.batchify.Stack())\n",
        "\n",
        "train_dataloader  = gluon.data.DataLoader(train, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
        "test_dataloader  = gluon.data.DataLoader(test, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
        "valid_dataloader  = gluon.data.DataLoader(valid, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt5gw0_dIXlP",
        "colab_type": "text"
      },
      "source": [
        "#### 모델링 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfxurevYIXlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EntityTagger(gluon.HybridBlock):\n",
        "    def __init__(self, vocab_size, vocab_out_size, num_embed, hidden_size, use_attention=False, **kwargs):\n",
        "        super(EntityTagger, self).__init__(**kwargs)\n",
        "        self.hidden_size = hidden_size \n",
        "        self.vocab_out_size = vocab_out_size\n",
        "        self.use_attention = use_attention\n",
        "        with self.name_scope():\n",
        "            self.embed = nn.Embedding(input_dim=vocab_size, output_dim=num_embed)\n",
        "            self.bigru = rnn.GRU(self.hidden_size, dropout=0.2, bidirectional=True)\n",
        "            self.dense_prev = nn.Dense(10, flatten=False)\n",
        "            self.dense = nn.Dense(self.vocab_out_size, flatten=False)\n",
        "            if self.use_attention:\n",
        "              self.attention = nlp.model.MLPAttentionCell(30, dropout=0.2)\n",
        "            \n",
        "    def hybrid_forward(self, F ,inputs, length):\n",
        "        em_out = self.embed(inputs)\n",
        "        bigruout = self.bigru(em_out)\n",
        "        masked_encoded = F.SequenceMask(bigruout,\n",
        "                                        sequence_length=length,\n",
        "                                        use_sequence_length=True).transpose((1,0,2))\n",
        "        if self.use_attention:\n",
        "            masked_encoded,_ = self.attention(masked_encoded, masked_encoded)\n",
        "        dense_out = self.dense_prev(masked_encoded)\n",
        "        outs = self.dense(dense_out) \n",
        "        return(outs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEWoD-4nIXla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu()\n",
        "\n",
        "model = EntityTagger(vocab_size = len(vocab_sent.idx_to_token), vocab_out_size=len(vocab_entity.idx_to_token), \n",
        "                     num_embed=50, hidden_size=30, use_attention=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBs0t8kcIXlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.initialize(mx.initializer.Xavier(), ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhMH6ZBMIXlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(model.collect_params(),\"Adam\")\n",
        "loss = gluon.loss.SoftmaxCELoss() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo8OAOnWIXls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.hybridize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0T8Pbg0IXlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "e51bff23-80aa-448f-eb5d-ebe4c8213f52"
      },
      "source": [
        "model"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EntityTagger(\n",
              "  (embed): Embedding(481 -> 50, float32)\n",
              "  (bigru): GRU(None -> 30, TNC, dropout=0.2, bidirectional)\n",
              "  (dense_prev): Dense(None -> 10, linear)\n",
              "  (dense): Dense(None -> 3, linear)\n",
              "  (attention): MLPAttentionCell(\n",
              "    (_act): Activation(tanh)\n",
              "    (_dropout_layer): Dropout(p = 0.2, axes=())\n",
              "    (_query_mid_layer): Dense(None -> 30, linear)\n",
              "    (_key_mid_layer): Dense(None -> 30, linear)\n",
              "    (_attention_score): Dense(30 -> 1, linear)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1NSWMLEIXl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
        "    corrected = 0\n",
        "    n = 0\n",
        "    for i, (data, length, label) in enumerate(data_iter):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        length = length.as_in_context(ctx)\n",
        "        output = model(data.T, length)\n",
        "        predictions = nd.argmax(output, axis=2)\n",
        "        tf = predictions.astype('int64') == label\n",
        "        for i in range(length.shape[0]):\n",
        "            l = int(length[i].asscalar())\n",
        "            corrected += nd.sum(tf[i][:l]).asscalar() == l\n",
        "            n += 1\n",
        "        #acc.update(preds=predictions, labels=label)\n",
        "    return(corrected/n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYE2pbAKIXl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
        "    test_loss = []\n",
        "    for i, (te_data, te_length, te_label) in enumerate(data_iter):\n",
        "        te_data = te_data.as_in_context(ctx)\n",
        "        te_label = te_label.as_in_context(ctx)\n",
        "        te_length = te_length.as_in_context(ctx)\n",
        "        te_output = model(te_data.T, te_length)\n",
        "        loss_te = loss_obj(te_output, te_label)\n",
        "        curr_loss = nd.mean(loss_te).asscalar()\n",
        "        test_loss.append(curr_loss)\n",
        "    return(np.mean(test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjuksSCWIXmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3f63be7-941c-4e67-f674-06fad143dc71"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "\n",
        "tot_test_loss = []\n",
        "tot_test_accu = []\n",
        "tot_train_loss = []\n",
        "tot_train_accu = []\n",
        "tot_valid_accu = [] \n",
        "for e in range(epochs):\n",
        "    #batch training \n",
        "    for i, (data, length, label) in enumerate(tqdm(train_dataloader)):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        length = length.as_in_context(ctx)\n",
        "        with autograd.record():\n",
        "            output = model(data.T, length)\n",
        "            loss_ = loss(output, label)\n",
        "            loss_.backward()\n",
        "        trainer.step(data.shape[0])\n",
        "\n",
        "    #caculate test loss\n",
        "    if e % 10 == 0: \n",
        "        test_loss = calculate_loss(model, test_dataloader, loss_obj = loss, ctx=ctx) \n",
        "        train_loss = calculate_loss(model, train_dataloader, loss_obj = loss, ctx=ctx) \n",
        "        test_accu = evaluate_accuracy(model, test_dataloader,  ctx=ctx)\n",
        "        train_accu = evaluate_accuracy(model, train_dataloader,  ctx=ctx)\n",
        "        valid_accu = evaluate_accuracy(model, valid_dataloader,  ctx=ctx)\n",
        "\n",
        "        print(\"Epoch %s. Train Loss: %s, Test Loss : %s,\" \\\n",
        "        \" Test Accuracy : %s,\" \\\n",
        "        \" Train Accuracy : %s : Valid Accuracy : %s\" % (e, train_loss, test_loss, test_accu, train_accu, valid_accu))    \n",
        "        tot_test_loss.append(test_loss)\n",
        "        tot_train_loss.append(train_loss)\n",
        "        tot_test_accu.append(test_accu)\n",
        "        tot_train_accu.append(train_accu)\n",
        "        tot_valid_accu.append(valid_accu)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 240.68it/s]\n",
            "  9%|▊         | 23/270 [00:00<00:01, 226.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 0.04581105, Test Loss : 0.04551146, Test Accuracy : 0.7133333333333334, Train Accuracy : 0.7050617283950618 : Valid Accuracy : 0.704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 175.59it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 191.02it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 167.02it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 184.27it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 202.12it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 181.01it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 176.99it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 182.01it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 197.47it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 196.37it/s]\n",
            "  7%|▋         | 19/270 [00:00<00:01, 188.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10. Train Loss: 0.0020585118, Test Loss : 0.00977939, Test Accuracy : 0.9533333333333334, Train Accuracy : 0.9849382716049383 : Valid Accuracy : 0.963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 178.87it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 193.61it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 180.71it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 193.57it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 186.82it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 192.00it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 179.13it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 188.45it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 206.38it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 192.83it/s]\n",
            "  9%|▉         | 24/270 [00:00<00:01, 238.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20. Train Loss: 0.0019672026, Test Loss : 0.010476824, Test Accuracy : 0.9588888888888889, Train Accuracy : 0.9837037037037037 : Valid Accuracy : 0.961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 200.94it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 175.08it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 185.28it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 176.77it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 175.91it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 186.16it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 184.73it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 183.85it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 195.25it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 178.42it/s]\n",
            "  8%|▊         | 22/270 [00:00<00:01, 219.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30. Train Loss: 0.0002923226, Test Loss : 0.0132401, Test Accuracy : 0.9611111111111111, Train Accuracy : 0.9969135802469136 : Valid Accuracy : 0.972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 178.83it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 180.42it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 185.86it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 204.22it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 179.28it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 191.57it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 177.67it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 191.65it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 198.73it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 198.85it/s]\n",
            "  9%|▊         | 23/270 [00:00<00:01, 229.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40. Train Loss: 3.341425e-05, Test Loss : 0.012503282, Test Accuracy : 0.9688888888888889, Train Accuracy : 0.9997530864197531 : Valid Accuracy : 0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 200.50it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 195.91it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 175.03it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 200.46it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 195.08it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 185.90it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 207.01it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 197.68it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 174.51it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 189.14it/s]\n",
            "  6%|▌         | 16/270 [00:00<00:01, 154.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50. Train Loss: 7.684146e-06, Test Loss : 0.013977752, Test Accuracy : 0.9711111111111111, Train Accuracy : 1.0 : Valid Accuracy : 0.982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 173.55it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 180.13it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 196.53it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 178.13it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 188.66it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 176.26it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 191.22it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 173.48it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 179.11it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 189.78it/s]\n",
            "  8%|▊         | 22/270 [00:00<00:01, 217.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 60. Train Loss: 0.0005088213, Test Loss : 0.014564858, Test Accuracy : 0.9644444444444444, Train Accuracy : 0.9950617283950617 : Valid Accuracy : 0.972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 170.08it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 190.07it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 174.73it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 170.88it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 189.08it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 201.39it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 176.72it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 183.48it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 167.09it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 187.38it/s]\n",
            "  7%|▋         | 20/270 [00:00<00:01, 198.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 70. Train Loss: 2.6472359e-05, Test Loss : 0.012694985, Test Accuracy : 0.9744444444444444, Train Accuracy : 0.9998765432098765 : Valid Accuracy : 0.983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 171.07it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 173.73it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 178.87it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 194.86it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 190.54it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 174.13it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 183.71it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 183.54it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 195.52it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 201.67it/s]\n",
            "  8%|▊         | 21/270 [00:00<00:01, 209.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 80. Train Loss: 3.950536e-06, Test Loss : 0.013301165, Test Accuracy : 0.98, Train Accuracy : 1.0 : Valid Accuracy : 0.983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 182.13it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 194.95it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 180.37it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 193.45it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 182.30it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 192.52it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 178.73it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 182.85it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 184.27it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 175.67it/s]\n",
            "  9%|▉         | 25/270 [00:00<00:01, 240.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 90. Train Loss: 1.37819925e-05, Test Loss : 0.009812501, Test Accuracy : 0.9822222222222222, Train Accuracy : 0.9997530864197531 : Valid Accuracy : 0.985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:01<00:00, 192.52it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 188.30it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 184.50it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 187.15it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 178.01it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 186.42it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 198.87it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 197.53it/s]\n",
            "100%|██████████| 270/270 [00:01<00:00, 182.71it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztk3C4GKLTgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.collect_params().reset_ctx(mx.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPLjG5IiIXmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_entitytag(sent):\n",
        "    sent_len = len(sent)\n",
        "    coded_sent = vocab_sent[length_clip(list(sent))]\n",
        "    co = nd.array(coded_sent).expand_dims(axis=1)\n",
        "    ret_code = model(co, nd.array([sent_len,]))\n",
        "    ret_seq = vocab_entity.to_tokens(ret_code.argmax(axis=2)[0].asnumpy().astype('int').tolist())\n",
        "    return(''.join(ret_seq)[:sent_len])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZKEW5WNIXmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "35dd56e0-4352-42a0-e3bb-f009b28bfed8"
      },
      "source": [
        "get_entitytag(\"파이콘이 뭔지 알려줘\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EEEECCCCCCC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD5giWUqIXmp",
        "colab_type": "text"
      },
      "source": [
        "### TODO\n",
        "- Test Accuracy 95% 이상 올리기\n",
        "- test_hidden 셋의 성능 90% 이상 올리기 \n",
        "- Entity Tagging과 Intent Classification을 MultiTask Learning으로 통합해보기(성능이 좋아지나? 나빠지나?)"
      ]
    }
  ]
}